{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e0a4962",
   "metadata": {},
   "source": [
    "# Investigation: Œª=0.0 for Consolidation\n",
    "\n",
    "## üî¨ Research Question\n",
    "\n",
    "**Finding**: Hyperparameter sensitivity analysis showed Œª=0.0 (no distillation during consolidation) performed **best** (90.61% ¬± 0.34%)\n",
    "\n",
    "**Current approach**: Uses Œª=0.5 during consolidation\n",
    "\n",
    "**Hypothesis**: Replay alone during consolidation might be more effective than replay+distillation\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Experiments\n",
    "\n",
    "1. **Split-MNIST (10 seeds)**: Compare Œª=0.0 vs Œª=0.5 consolidation\n",
    "2. **Split-CIFAR-10 (5 seeds)**: Validate if Œª=0.0 improves CIFAR-10 results too\n",
    "3. **Analysis**: Why does no distillation work better during consolidation?\n",
    "\n",
    "---\n",
    "\n",
    "**Date**: November 9, 2025  \n",
    "**Priority**: HIGH (potential paper improvement)  \n",
    "**Expected Runtime**: ~1.5 hours on Colab T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdc7dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "from scipy import stats\n",
    "\n",
    "from src.models import (\n",
    "    NN1_SimpleMLP,\n",
    "    NN2_ConsolidationNet,\n",
    "    ReplayBuffer,\n",
    "    evaluate_models,\n",
    "    train_task_with_replay,\n",
    "    consolidate_nn2\n",
    ")\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"üñ•Ô∏è  Device: {device}\")\n",
    "print(f\"üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üî• PyTorch: {torch.__version__}\")\n",
    "print(\"\\n‚≠ê Œª=0.0 Investigation ‚≠ê\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb16487",
   "metadata": {},
   "source": [
    "> ‚ÑπÔ∏è The shared training utilities now provide task-balanced replay sampling and support Œª=0.0 without computing teacher logits, so this notebook mirrors the production code path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4ffcc9",
   "metadata": {},
   "source": [
    "## 1. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af798a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST\n",
    "transform_mnist = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "mnist_train = datasets.MNIST('../../data', train=True, download=True, transform=transform_mnist)\n",
    "mnist_test = datasets.MNIST('../../data', train=False, download=True, transform=transform_mnist)\n",
    "\n",
    "# CIFAR-10\n",
    "transform_cifar_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_cifar_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "cifar_train = datasets.CIFAR10('../../data', train=True, download=True, transform=transform_cifar_train)\n",
    "cifar_test = datasets.CIFAR10('../../data', train=False, download=True, transform=transform_cifar_test)\n",
    "\n",
    "print(\"‚úÖ Datasets loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166db86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create task splits\n",
    "def create_task_split(dataset, class_pairs):\n",
    "    indices = []\n",
    "    for idx in range(len(dataset)):\n",
    "        if hasattr(dataset, 'targets'):\n",
    "            label = dataset.targets[idx]\n",
    "        else:\n",
    "            _, label = dataset[idx]\n",
    "        if label in class_pairs:\n",
    "            indices.append(idx)\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "# MNIST tasks\n",
    "mnist_tasks_def = [\n",
    "    ([0, 1], \"Task 1: 0-1\"),\n",
    "    ([2, 3], \"Task 2: 2-3\"),\n",
    "    ([4, 5], \"Task 3: 4-5\"),\n",
    "    ([6, 7], \"Task 4: 6-7\"),\n",
    "    ([8, 9], \"Task 5: 8-9\"),\n",
    "]\n",
    "\n",
    "mnist_train_tasks = [create_task_split(mnist_train, digits) for digits, _ in mnist_tasks_def]\n",
    "mnist_test_tasks = [create_task_split(mnist_test, digits) for digits, _ in mnist_tasks_def]\n",
    "\n",
    "# CIFAR-10 tasks\n",
    "cifar_tasks_def = [\n",
    "    ([0, 1], \"Task 1: airplane, automobile\"),\n",
    "    ([2, 3], \"Task 2: bird, cat\"),\n",
    "    ([4, 5], \"Task 3: deer, dog\"),\n",
    "    ([6, 7], \"Task 4: frog, horse\"),\n",
    "    ([8, 9], \"Task 5: ship, truck\"),\n",
    "]\n",
    "\n",
    "cifar_train_tasks = [create_task_split(cifar_train, classes) for classes, _ in cifar_tasks_def]\n",
    "cifar_test_tasks = [create_task_split(cifar_test, classes) for classes, _ in cifar_tasks_def]\n",
    "\n",
    "print(\"‚úÖ Task splits created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3c52c6",
   "metadata": {},
   "source": [
    "## 2. Experiment Function (Configurable Œª)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe09815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(dataset='mnist', seed=42, consolidation_lambda=0.5, verbose=False):\n",
    "    \"\"\"\n",
    "    Run FSC-Net with configurable consolidation lambda\n",
    "    \n",
    "    Args:\n",
    "        dataset: 'mnist' or 'cifar10'\n",
    "        seed: Random seed\n",
    "        consolidation_lambda: Lambda for consolidation phase (0.0 = no distillation)\n",
    "        verbose: Print progress\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Select dataset\n",
    "    if dataset == 'mnist':\n",
    "        train_tasks = mnist_train_tasks\n",
    "        test_tasks = mnist_test_tasks\n",
    "        in_dim = 784\n",
    "    else:\n",
    "        train_tasks = cifar_train_tasks\n",
    "        test_tasks = cifar_test_tasks\n",
    "        in_dim = 3072\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üå± {dataset.upper()} | Seed {seed} | Consolidation Œª={consolidation_lambda}\")\n",
    "        print(f\"{'='*60}\")\n",
    "    \n",
    "    # Initialize models\n",
    "    nn1 = NN1_SimpleMLP(in_dim=in_dim, neuron_dim=64, num_classes=10).to(device)\n",
    "    nn2 = NN2_ConsolidationNet(in_dim=in_dim, summary_dim=64, num_classes=10).to(device)\n",
    "    \n",
    "    opt1 = torch.optim.Adam(nn1.parameters(), lr=1e-3)\n",
    "    opt2 = torch.optim.Adam(nn2.parameters(), lr=5e-4)\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
    "    \n",
    "    replay_buffer = ReplayBuffer(buffer_size_per_task=200)\n",
    "    \n",
    "    # Train on each task\n",
    "    for task_id, train_task in enumerate(train_tasks):\n",
    "        train_loader = DataLoader(train_task, batch_size=64, shuffle=True)\n",
    "        \n",
    "        # Task training (use Œª=0.3 for task training - this stays the same)\n",
    "        train_task_with_replay(\n",
    "            nn1, nn2, train_loader, replay_buffer.get_dataset(),\n",
    "            opt1, opt2, ce_loss, kl_loss,\n",
    "            device=device, epochs=5,\n",
    "            lambda_distill=0.3,  # Keep task training lambda at 0.3\n",
    "            temperature=2.0\n",
    "        )\n",
    "        \n",
    "        replay_buffer.add_task(train_task)\n",
    "        \n",
    "        # Offline consolidation (TEST different lambda here)\n",
    "        consolidate_nn2(\n",
    "            nn1, nn2, replay_buffer.get_dataset(),\n",
    "            opt2, ce_loss, kl_loss,\n",
    "            device=device,\n",
    "            consolidation_epochs=2,\n",
    "            lambda_distill=consolidation_lambda,  # TEST PARAMETER\n",
    "            temperature=2.0\n",
    "        )\n",
    "    \n",
    "    # Final evaluation\n",
    "    all_test_data = []\n",
    "    for task in test_tasks:\n",
    "        all_test_data.extend(task)\n",
    "    \n",
    "    test_loader = DataLoader(all_test_data, batch_size=128, shuffle=False)\n",
    "    acc1, acc2 = evaluate_models(nn1, nn2, test_loader, device=device)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"üìä Final: NN1={acc1*100:.2f}%, NN2={acc2*100:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'dataset': dataset,\n",
    "        'seed': seed,\n",
    "        'consolidation_lambda': consolidation_lambda,\n",
    "        'nn1_final': acc1 * 100,\n",
    "        'nn2_final': acc2 * 100,\n",
    "        'improvement': (acc2 - acc1) * 100\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Experiment function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c43619",
   "metadata": {},
   "source": [
    "## 3. Experiment 1: MNIST Comparison (10 seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54b1a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Œª=0.0 vs Œª=0.5 on MNIST\n",
    "seeds = list(range(42, 52))  # 10 seeds\n",
    "lambdas = [0.0, 0.5]\n",
    "\n",
    "print(\"üî¨ MNIST: Œª=0.0 vs Œª=0.5 (10 seeds)\\n\")\n",
    "\n",
    "mnist_results = []\n",
    "\n",
    "for lam in lambdas:\n",
    "    print(f\"\\nüìä Testing Œª={lam}...\")\n",
    "    for seed in tqdm(seeds, desc=f\"Œª={lam}\"):\n",
    "        result = run_experiment(\n",
    "            dataset='mnist',\n",
    "            seed=seed,\n",
    "            consolidation_lambda=lam,\n",
    "            verbose=False\n",
    "        )\n",
    "        mnist_results.append(result)\n",
    "\n",
    "df_mnist = pd.DataFrame(mnist_results)\n",
    "print(\"\\n‚úÖ MNIST experiments complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba69aa9",
   "metadata": {},
   "source": [
    "## 4. Experiment 2: CIFAR-10 Comparison (5 seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1134c4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Œª=0.0 vs Œª=0.5 on CIFAR-10\n",
    "seeds = [42, 43, 44, 45, 46]  # 5 seeds (same as validation)\n",
    "lambdas = [0.0, 0.5]\n",
    "\n",
    "print(\"üî¨ CIFAR-10: Œª=0.0 vs Œª=0.5 (5 seeds)\\n\")\n",
    "\n",
    "cifar_results = []\n",
    "\n",
    "for lam in lambdas:\n",
    "    print(f\"\\nüìä Testing Œª={lam}...\")\n",
    "    for seed in tqdm(seeds, desc=f\"Œª={lam}\"):\n",
    "        result = run_experiment(\n",
    "            dataset='cifar10',\n",
    "            seed=seed,\n",
    "            consolidation_lambda=lam,\n",
    "            verbose=False\n",
    "        )\n",
    "        cifar_results.append(result)\n",
    "\n",
    "df_cifar = pd.DataFrame(cifar_results)\n",
    "print(\"\\n‚úÖ CIFAR-10 experiments complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdbb0fd",
   "metadata": {},
   "source": [
    "## 5. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de97106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(df, dataset_name):\n",
    "    \"\"\"Analyze and compare Œª=0.0 vs Œª=0.5\"\"\"\n",
    "    \n",
    "    lambda_0 = df[df['consolidation_lambda'] == 0.0]['nn2_final'].values\n",
    "    lambda_5 = df[df['consolidation_lambda'] == 0.5]['nn2_final'].values\n",
    "    \n",
    "    mean_0, std_0 = np.mean(lambda_0), np.std(lambda_0, ddof=1)\n",
    "    mean_5, std_5 = np.mean(lambda_5), np.std(lambda_5, ddof=1)\n",
    "    \n",
    "    # Paired t-test\n",
    "    t_stat, p_value = stats.ttest_rel(lambda_0, lambda_5)\n",
    "    \n",
    "    # Effect size (Cohen's d)\n",
    "    diff = lambda_0 - lambda_5\n",
    "    cohens_d = np.mean(diff) / np.std(diff, ddof=1)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{dataset_name} RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(f\"\\nüìä Œª=0.0 (No distillation during consolidation):\")\n",
    "    print(f\"   Mean: {mean_0:.2f}%\")\n",
    "    print(f\"   Std:  {std_0:.2f}%\")\n",
    "    print(f\"   Values: {[f'{v:.2f}' for v in lambda_0]}\")\n",
    "    \n",
    "    print(f\"\\nüìä Œª=0.5 (Current approach):\")\n",
    "    print(f\"   Mean: {mean_5:.2f}%\")\n",
    "    print(f\"   Std:  {std_5:.2f}%\")\n",
    "    print(f\"   Values: {[f'{v:.2f}' for v in lambda_5]}\")\n",
    "    \n",
    "    print(f\"\\nüìà Comparison:\")\n",
    "    print(f\"   Difference: {mean_0 - mean_5:+.2f}%\")\n",
    "    print(f\"   t-statistic: {t_stat:.3f}\")\n",
    "    print(f\"   p-value: {p_value:.4f}\")\n",
    "    print(f\"   Cohen's d: {cohens_d:.3f}\")\n",
    "    print(f\"   Significant (p<0.05): {'YES ‚úÖ' if p_value < 0.05 else 'NO ‚ùå'}\")\n",
    "    \n",
    "    if mean_0 > mean_5:\n",
    "        print(f\"\\nüéØ Verdict: Œª=0.0 is {'SIGNIFICANTLY ' if p_value < 0.05 else ''}BETTER by {mean_0 - mean_5:.2f}%\")\n",
    "    else:\n",
    "        print(f\"\\nüéØ Verdict: Œª=0.5 is {'SIGNIFICANTLY ' if p_value < 0.05 else ''}BETTER by {mean_5 - mean_0:.2f}%\")\n",
    "    \n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'mean_lambda0': mean_0,\n",
    "        'std_lambda0': std_0,\n",
    "        'mean_lambda5': mean_5,\n",
    "        'std_lambda5': std_5,\n",
    "        'difference': mean_0 - mean_5,\n",
    "        'p_value': p_value,\n",
    "        'cohens_d': cohens_d\n",
    "    }\n",
    "\n",
    "mnist_stats = analyze_results(df_mnist, \"MNIST\")\n",
    "cifar_stats = analyze_results(df_cifar, \"CIFAR-10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ebc8e7",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491991d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# MNIST comparison\n",
    "ax = axes[0]\n",
    "mnist_lambda0 = df_mnist[df_mnist['consolidation_lambda'] == 0.0]['nn2_final'].values\n",
    "mnist_lambda5 = df_mnist[df_mnist['consolidation_lambda'] == 0.5]['nn2_final'].values\n",
    "\n",
    "bp = ax.boxplot([mnist_lambda5, mnist_lambda0], positions=[1, 2], widths=0.6,\n",
    "                 patch_artist=True, showmeans=True)\n",
    "for patch, color in zip(bp['boxes'], ['lightcoral', 'lightgreen']):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "ax.set_xticks([1, 2])\n",
    "ax.set_xticklabels(['Œª=0.5\\n(Current)', 'Œª=0.0\\n(No Distill)'], fontweight='bold')\n",
    "ax.set_ylabel('NN2 Retention (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title(f'MNIST: Œª=0.0 vs Œª=0.5\\n(p={mnist_stats[\"p_value\"]:.4f})', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add significance annotation\n",
    "if mnist_stats['p_value'] < 0.05:\n",
    "    y_max = max(max(mnist_lambda0), max(mnist_lambda5))\n",
    "    ax.plot([1, 2], [y_max + 0.5, y_max + 0.5], 'k-', linewidth=1.5)\n",
    "    ax.text(1.5, y_max + 0.7, '***' if mnist_stats['p_value'] < 0.001 else '**' if mnist_stats['p_value'] < 0.01 else '*',\n",
    "            ha='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "# CIFAR-10 comparison\n",
    "ax = axes[1]\n",
    "cifar_lambda0 = df_cifar[df_cifar['consolidation_lambda'] == 0.0]['nn2_final'].values\n",
    "cifar_lambda5 = df_cifar[df_cifar['consolidation_lambda'] == 0.5]['nn2_final'].values\n",
    "\n",
    "bp = ax.boxplot([cifar_lambda5, cifar_lambda0], positions=[1, 2], widths=0.6,\n",
    "                 patch_artist=True, showmeans=True)\n",
    "for patch, color in zip(bp['boxes'], ['lightcoral', 'lightgreen']):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "ax.set_xticks([1, 2])\n",
    "ax.set_xticklabels(['Œª=0.5\\n(Current)', 'Œª=0.0\\n(No Distill)'], fontweight='bold')\n",
    "ax.set_ylabel('NN2 Retention (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title(f'CIFAR-10: Œª=0.0 vs Œª=0.5\\n(p={cifar_stats[\"p_value\"]:.4f})', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add significance annotation\n",
    "if cifar_stats['p_value'] < 0.05:\n",
    "    y_max = max(max(cifar_lambda0), max(cifar_lambda5))\n",
    "    ax.plot([1, 2], [y_max + 0.5, y_max + 0.5], 'k-', linewidth=1.5)\n",
    "    ax.text(1.5, y_max + 0.7, '***' if cifar_stats['p_value'] < 0.001 else '**' if cifar_stats['p_value'] < 0.01 else '*',\n",
    "            ha='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "plt.savefig(f'../../results/simple_mlp/figures/lambda_zero_investigation_{timestamp}.png',\n",
    "            dpi=300, bbox_inches='tight')\n",
    "print(f\"üíæ Saved: results/simple_mlp/figures/lambda_zero_investigation_{timestamp}.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c10027f",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77392e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine results\n",
    "df_all = pd.concat([df_mnist, df_cifar], ignore_index=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "csv_path = f'../../results/simple_mlp/csv/lambda_zero_investigation_{timestamp}.csv'\n",
    "df_all.to_csv(csv_path, index=False)\n",
    "print(f\"üíæ Saved: {csv_path}\")\n",
    "\n",
    "print(\"\\nüìä Combined Results:\")\n",
    "print(df_all.groupby(['dataset', 'consolidation_lambda'])['nn2_final'].agg(['mean', 'std', 'count']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e135517a",
   "metadata": {},
   "source": [
    "## 8. Analysis: Why Does Œª=0.0 Work Better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975125e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ANALYSIS: WHY DOES Œª=0.0 WORK BETTER DURING CONSOLIDATION?\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüî¨ Possible Explanations:\\n\")\n",
    "\n",
    "print(\"1. üìö REPLAY IS SUFFICIENT:\")\n",
    "print(\"   - Replay buffer already contains diverse examples from all tasks\")\n",
    "print(\"   - NN2 learns directly from ground truth labels (via CE loss)\")\n",
    "print(\"   - Distillation from NN1 may introduce noise/bias from fast learning\")\n",
    "\n",
    "print(\"\\n2. ‚ö° NN1 IS TOO TASK-SPECIFIC:\")\n",
    "print(\"   - NN1 adapts quickly to current task (high learning rate)\")\n",
    "print(\"   - NN1's predictions may be overfit to recent tasks\")\n",
    "print(\"   - Distilling from NN1 transfers this recency bias to NN2\")\n",
    "\n",
    "print(\"\\n3. üéØ CONSOLIDATION PHASE IS DIFFERENT:\")\n",
    "print(\"   - During task training: Distillation helps (Œª=0.3 is beneficial)\")\n",
    "print(\"   - During consolidation: Only replay data, no new task interference\")\n",
    "print(\"   - Direct learning from labels may be cleaner than distillation\")\n",
    "\n",
    "print(\"\\n4. üîÑ DISTILLATION TEMPERATURE MISMATCH:\")\n",
    "print(\"   - Temperature T=2.0 may not be optimal for consolidation phase\")\n",
    "print(\"   - Higher temperature softens targets, may lose important distinctions\")\n",
    "\n",
    "print(\"\\n5. üìä OPTIMIZATION DYNAMICS:\")\n",
    "print(\"   - Pure CE loss: Clear gradient signal from labels\")\n",
    "print(\"   - Mixed CE+KL loss: Gradient conflict between objectives\")\n",
    "print(\"   - NN2 already benefits from NN1's summary embedding (input fusion)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RECOMMENDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if mnist_stats['difference'] > 0 and mnist_stats['p_value'] < 0.05:\n",
    "    print(\"\\n‚úÖ UPDATE PAPER: Use Œª=0.0 for consolidation phase\")\n",
    "    print(\"\\n   Modified training protocol:\")\n",
    "    print(\"   ‚Ä¢ Task training: Œª=0.3 (keep distillation)\")\n",
    "    print(\"   ‚Ä¢ Consolidation: Œª=0.0 (pure replay, no distillation)\")\n",
    "    print(f\"\\n   Expected improvement:\")\n",
    "    print(f\"   ‚Ä¢ MNIST: +{mnist_stats['difference']:.2f}%\")\n",
    "    if cifar_stats['difference'] > 0:\n",
    "        print(f\"   ‚Ä¢ CIFAR-10: +{cifar_stats['difference']:.2f}%\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è KEEP CURRENT: Œª=0.5 performs similarly or better\")\n",
    "    print(\"   No significant benefit from switching to Œª=0.0\")\n",
    "\n",
    "print(\"\\nüéâ Investigation Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5280fc",
   "metadata": {},
   "source": [
    "## 9. Recorded Output (Colab Run, Nov 10, 2025)\n",
    "```\n",
    "============================================================\n",
    "MNIST RESULTS\n",
    "============================================================\n",
    "\n",
    "üìä Œª=0.0 (No distillation during consolidation):\n",
    "   Mean: 91.46%\n",
    "   Std:  0.84%\n",
    "   Values: ['90.39', '90.45', '91.00', '92.48', '91.10', '92.28', '92.13', '90.55', '92.12', '92.07']\n",
    "\n",
    "üìä Œª=0.5 (Current approach):\n",
    "   Mean: 90.20%\n",
    "   Std:  1.67%\n",
    "   Values: ['90.30', '88.74', '86.06', '91.23', '90.34', '91.12', '91.04', '90.58', '91.90', '90.70']\n",
    "\n",
    "üìà Comparison:\n",
    "   Difference: +1.26%\n",
    "   t-statistic: 2.796\n",
    "   p-value: 0.0208\n",
    "   Cohen's d: 0.884\n",
    "   Significant (p<0.05): YES ‚úÖ\n",
    "\n",
    "üéØ Verdict: Œª=0.0 is SIGNIFICANTLY BETTER by 1.26%\n",
    "============================================================\n",
    "\n",
    "\n",
    "============================================================\n",
    "CIFAR-10 RESULTS\n",
    "============================================================\n",
    "\n",
    "üìä Œª=0.0 (No distillation during consolidation):\n",
    "   Mean: 34.38%\n",
    "   Std:  0.67%\n",
    "   Values: ['34.22', '33.64', '35.48', '34.34', '34.22']\n",
    "\n",
    "üìä Œª=0.5 (Current approach):\n",
    "   Mean: 32.62%\n",
    "   Std:  1.70%\n",
    "   Values: ['33.34', '32.77', '34.07', '33.22', '29.70']\n",
    "\n",
    "üìà Comparison:\n",
    "   Difference: +1.76%\n",
    "   t-statistic: 2.525\n",
    "   p-value: 0.0650\n",
    "   Cohen's d: 1.129\n",
    "   Significant (p<0.05): NO ‚ùå\n",
    "\n",
    "üéØ Verdict: Œª=0.0 is BETTER by 1.76%\n",
    "============================================================\n",
    "\n",
    "üíæ Saved: /content/lambda_zero_investigation_20251110_194805.csv\n",
    "\n",
    "üìä Combined Results:\n",
    "                                mean       std  count\n",
    "dataset consolidation_lambda                         \n",
    "cifar10 0.0                   34.380  0.672756      5\n",
    "        0.5                   32.620  1.697778      5\n",
    "mnist   0.0                   91.457  0.836900     10\n",
    "        0.5                   90.201  1.673257     10\n",
    "\n",
    "======================================================================\n",
    "ANALYSIS: WHY DOES Œª=0.0 WORK BETTER DURING CONSOLIDATION?\n",
    "======================================================================\n",
    "\n",
    "üî¨ Possible Explanations:\n",
    "\n",
    "1. üìö REPLAY IS SUFFICIENT:\n",
    "   - Replay buffer already contains diverse examples from all tasks\n",
    "   - NN2 learns directly from ground truth labels (via CE loss)\n",
    "   - Distillation from NN1 may introduce noise/bias from fast learning\n",
    "\n",
    "2. ‚ö° NN1 IS TOO TASK-SPECIFIC:\n",
    "   - NN1 adapts quickly to current task (high learning rate)\n",
    "   - NN1's predictions may be overfit to recent tasks\n",
    "   - Distilling from NN1 transfers this recency bias to NN2\n",
    "\n",
    "3. üéØ CONSOLIDATION PHASE IS DIFFERENT:\n",
    "   - During task training: Distillation helps (Œª=0.3 is beneficial)\n",
    "   - During consolidation: Only replay data, no new task interference\n",
    "   - Direct learning from labels may be cleaner than distillation\n",
    "\n",
    "4. üîÑ DISTILLATION TEMPERATURE MISMATCH:\n",
    "   - Temperature T=2.0 may not be optimal for consolidation phase\n",
    "   - Higher temperature softens targets, may lose important distinctions\n",
    "\n",
    "5. üìä OPTIMIZATION DYNAMICS:\n",
    "   - Pure CE loss: Clear gradient signal from labels\n",
    "   - Mixed CE+KL loss: Gradient conflict between objectives\n",
    "   - NN2 already benefits from NN1's summary embedding (input fusion)\n",
    "\n",
    "======================================================================\n",
    "RECOMMENDATION\n",
    "======================================================================\n",
    "\n",
    "‚úÖ UPDATE PAPER: Use Œª=0.0 for consolidation phase\n",
    "\n",
    "   Modified training protocol:\n",
    "   ‚Ä¢ Task training: Œª=0.3 (keep distillation)\n",
    "   ‚Ä¢ Consolidation: Œª=0.0 (pure replay, no distillation)\n",
    "\n",
    "   Expected improvement:\n",
    "   ‚Ä¢ MNIST: +1.26%\n",
    "   ‚Ä¢ CIFAR-10: +1.76%\n",
    "\n",
    "üéâ Investigation Complete!\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
