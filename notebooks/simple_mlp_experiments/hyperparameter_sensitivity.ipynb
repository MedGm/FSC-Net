{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5abd1d5",
   "metadata": {},
   "source": [
    "# Hyperparameter Sensitivity Analysis\n",
    "\n",
    "## üéØ Goal: Generate Sensitivity Plots for Paper\n",
    "\n",
    "**Purpose**: Test robustness of FSC-Net to hyperparameter choices\n",
    "\n",
    "**Hyperparameters to Test**:\n",
    "1. **Distillation weight Œª**: [0.0, 0.2, 0.3, 0.4, 0.5, 0.6, 0.8, 1.0]\n",
    "2. **Replay buffer size**: [50, 100, 200, 300, 400, 500]\n",
    "\n",
    "**Expected Findings**:\n",
    "- Performance robust to Œª ‚àà [0.3, 0.6]\n",
    "- Diminishing returns beyond 200 samples/task\n",
    "\n",
    "**Output**: Two plots for Figure 4.6 in paper\n",
    "\n",
    "---\n",
    "\n",
    "**Date**: November 9, 2025  \n",
    "**Priority**: HIGH (needed for paper figures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9712de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.models import (\n",
    "    NN1_SimpleMLP,\n",
    "    NN2_ConsolidationNet,\n",
    "    ReplayBuffer,\n",
    "    evaluate_models,\n",
    "    train_task_with_replay,\n",
    "    consolidate_nn2\n",
    ")\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"üñ•Ô∏è  Device: {device}\")\n",
    "print(f\"üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üî• PyTorch: {torch.__version__}\")\n",
    "print(\"\\n‚≠ê Hyperparameter Sensitivity Analysis ‚≠ê\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b868dc0",
   "metadata": {},
   "source": [
    "## 1. Load Split-MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf97ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('../../data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('../../data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split into 5 tasks (2 digits each)\n",
    "def create_task_split(dataset, digit_pairs):\n",
    "    indices = []\n",
    "    for idx, (img, label) in enumerate(dataset):\n",
    "        if label in digit_pairs:\n",
    "            indices.append(idx)\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "tasks = [\n",
    "    ([0, 1], \"Task 1: Digits 0-1\"),\n",
    "    ([2, 3], \"Task 2: Digits 2-3\"),\n",
    "    ([4, 5], \"Task 3: Digits 4-5\"),\n",
    "    ([6, 7], \"Task 4: Digits 6-7\"),\n",
    "    ([8, 9], \"Task 5: Digits 8-9\"),\n",
    "]\n",
    "\n",
    "train_tasks = [create_task_split(train_dataset, digits) for digits, _ in tasks]\n",
    "test_tasks = [create_task_split(test_dataset, digits) for digits, _ in tasks]\n",
    "\n",
    "print(\"‚úÖ Split-MNIST Created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b01c919",
   "metadata": {},
   "source": [
    "## 2. Experiment Function with Configurable Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dc622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(seed=42, lambda_distill=0.5, buffer_size=200, verbose=False):\n",
    "    \"\"\"\n",
    "    Run continual learning experiment with specific hyperparameters\n",
    "    \n",
    "    Args:\n",
    "        seed: Random seed\n",
    "        lambda_distill: Distillation weight for consolidation phase\n",
    "        buffer_size: Replay buffer size per task\n",
    "        verbose: Print progress\n",
    "    \n",
    "    Returns:\n",
    "        dict with NN1 and NN2 final retention\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Initialize models\n",
    "    nn1 = NN1_SimpleMLP(in_dim=784, neuron_dim=64, num_classes=10).to(device)\n",
    "    nn2 = NN2_ConsolidationNet(in_dim=784, summary_dim=64, num_classes=10).to(device)\n",
    "    \n",
    "    opt1 = torch.optim.Adam(nn1.parameters(), lr=1e-3)\n",
    "    opt2 = torch.optim.Adam(nn2.parameters(), lr=5e-4)\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
    "    \n",
    "    replay_buffer = ReplayBuffer(buffer_size_per_task=buffer_size)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nüîß Config: Œª={lambda_distill}, buffer={buffer_size}, seed={seed}\")\n",
    "    \n",
    "    # Train on each task\n",
    "    for task_id, (train_task, test_task) in enumerate(zip(train_tasks, test_tasks)):\n",
    "        train_loader = DataLoader(train_task, batch_size=64, shuffle=True)\n",
    "        \n",
    "        # Task training with replay\n",
    "        train_task_with_replay(\n",
    "            nn1, nn2, train_loader, replay_buffer.get_dataset(),\n",
    "            opt1, opt2, ce_loss, kl_loss,\n",
    "            device=device, epochs=5,\n",
    "            lambda_distill=0.3,  # Use default for task training\n",
    "            temperature=2.0\n",
    "        )\n",
    "        \n",
    "        replay_buffer.add_task(train_task)\n",
    "        \n",
    "        # Offline consolidation with TEST lambda\n",
    "        consolidate_nn2(\n",
    "            nn1, nn2, replay_buffer.get_dataset(),\n",
    "            opt2, ce_loss, kl_loss,\n",
    "            device=device,\n",
    "            consolidation_epochs=2,\n",
    "            lambda_distill=lambda_distill,  # TEST PARAMETER\n",
    "            temperature=2.0\n",
    "        )\n",
    "    \n",
    "    # Final evaluation on all tasks\n",
    "    all_test_data = []\n",
    "    for task in test_tasks:\n",
    "        all_test_data.extend(task)\n",
    "    \n",
    "    test_loader = DataLoader(all_test_data, batch_size=128, shuffle=False)\n",
    "    acc1, acc2 = evaluate_models(nn1, nn2, test_loader, device=device)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"   üìä Final: NN1={acc1*100:.2f}%, NN2={acc2*100:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'nn1_retention': acc1 * 100,\n",
    "        'nn2_retention': acc2 * 100,\n",
    "        'lambda': lambda_distill,\n",
    "        'buffer_size': buffer_size,\n",
    "        'seed': seed\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Experiment function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf7b437",
   "metadata": {},
   "source": [
    "## 3. Experiment 1: Distillation Weight Œª Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f797223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different lambda values\n",
    "lambda_values = [0.0, 0.2, 0.3, 0.4, 0.5, 0.6, 0.8, 1.0]\n",
    "seeds = [42, 43, 44]  # 3 seeds for robustness\n",
    "\n",
    "print(\"üî¨ Testing distillation weight Œª...\")\n",
    "print(f\"   Values: {lambda_values}\")\n",
    "print(f\"   Seeds: {seeds}\")\n",
    "\n",
    "lambda_results = []\n",
    "\n",
    "for lam in tqdm(lambda_values, desc=\"Lambda values\"):\n",
    "    for seed in seeds:\n",
    "        result = run_experiment(\n",
    "            seed=seed,\n",
    "            lambda_distill=lam,\n",
    "            buffer_size=200,  # Default\n",
    "            verbose=False\n",
    "        )\n",
    "        lambda_results.append(result)\n",
    "\n",
    "df_lambda = pd.DataFrame(lambda_results)\n",
    "print(\"\\n‚úÖ Lambda sensitivity complete!\")\n",
    "print(df_lambda.groupby('lambda')[['nn1_retention', 'nn2_retention']].agg(['mean', 'std']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60d9492",
   "metadata": {},
   "source": [
    "## 4. Experiment 2: Replay Buffer Size Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d07bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different buffer sizes\n",
    "buffer_sizes = [50, 100, 200, 300, 400, 500]\n",
    "seeds = [42, 43, 44]  # 3 seeds\n",
    "\n",
    "print(\"üî¨ Testing replay buffer size...\")\n",
    "print(f\"   Sizes: {buffer_sizes}\")\n",
    "print(f\"   Seeds: {seeds}\")\n",
    "\n",
    "buffer_results = []\n",
    "\n",
    "for size in tqdm(buffer_sizes, desc=\"Buffer sizes\"):\n",
    "    for seed in seeds:\n",
    "        result = run_experiment(\n",
    "            seed=seed,\n",
    "            lambda_distill=0.5,  # Default\n",
    "            buffer_size=size,\n",
    "            verbose=False\n",
    "        )\n",
    "        buffer_results.append(result)\n",
    "\n",
    "df_buffer = pd.DataFrame(buffer_results)\n",
    "print(\"\\n‚úÖ Buffer sensitivity complete!\")\n",
    "print(df_buffer.groupby('buffer_size')[['nn1_retention', 'nn2_retention']].agg(['mean', 'std']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea8e825",
   "metadata": {},
   "source": [
    "## 5. Generate Plots for Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122baaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with two subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Lambda sensitivity\n",
    "ax = axes[0]\n",
    "lambda_summary = df_lambda.groupby('lambda').agg({'nn2_retention': ['mean', 'std']}).reset_index()\n",
    "lambda_summary.columns = ['lambda', 'mean', 'std']\n",
    "\n",
    "ax.errorbar(lambda_summary['lambda'], lambda_summary['mean'], \n",
    "            yerr=lambda_summary['std'],\n",
    "            marker='o', markersize=8, capsize=5, capthick=2,\n",
    "            linewidth=2, color='steelblue', label='NN2 (Consolidation)')\n",
    "\n",
    "# Highlight optimal range\n",
    "ax.axvspan(0.3, 0.6, alpha=0.2, color='green', label='Recommended range')\n",
    "\n",
    "ax.set_xlabel('Distillation Weight Œª', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Retention Accuracy (%)', fontsize=13, fontweight='bold')\n",
    "ax.set_title('(a) Distillation Weight Sensitivity', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim([85, 93])\n",
    "\n",
    "# Plot 2: Buffer size sensitivity\n",
    "ax = axes[1]\n",
    "buffer_summary = df_buffer.groupby('buffer_size').agg({'nn2_retention': ['mean', 'std']}).reset_index()\n",
    "buffer_summary.columns = ['buffer_size', 'mean', 'std']\n",
    "\n",
    "ax.errorbar(buffer_summary['buffer_size'], buffer_summary['mean'],\n",
    "            yerr=buffer_summary['std'],\n",
    "            marker='s', markersize=8, capsize=5, capthick=2,\n",
    "            linewidth=2, color='coral', label='NN2 (Consolidation)')\n",
    "\n",
    "# Highlight diminishing returns\n",
    "ax.axvline(200, color='green', linestyle='--', linewidth=2, alpha=0.7, label='Default (200)')\n",
    "\n",
    "ax.set_xlabel('Replay Buffer Size (samples/task)', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Retention Accuracy (%)', fontsize=13, fontweight='bold')\n",
    "ax.set_title('(b) Replay Buffer Size Sensitivity', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim([85, 93])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save individual plots for paper\n",
    "fig1, ax1 = plt.subplots(figsize=(7, 5))\n",
    "ax1.errorbar(lambda_summary['lambda'], lambda_summary['mean'], \n",
    "            yerr=lambda_summary['std'],\n",
    "            marker='o', markersize=10, capsize=6, capthick=2.5,\n",
    "            linewidth=2.5, color='steelblue')\n",
    "ax1.axvspan(0.3, 0.6, alpha=0.15, color='green')\n",
    "ax1.set_xlabel('Distillation Weight Œª', fontsize=14)\n",
    "ax1.set_ylabel('Retention Accuracy (%)', fontsize=14)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim([85, 93])\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../figures/hyperparameter_lambda.png', dpi=300, bbox_inches='tight')\n",
    "print(\"üíæ Saved: figures/hyperparameter_lambda.png\")\n",
    "\n",
    "fig2, ax2 = plt.subplots(figsize=(7, 5))\n",
    "ax2.errorbar(buffer_summary['buffer_size'], buffer_summary['mean'],\n",
    "            yerr=buffer_summary['std'],\n",
    "            marker='s', markersize=10, capsize=6, capthick=2.5,\n",
    "            linewidth=2.5, color='coral')\n",
    "ax2.axvline(200, color='green', linestyle='--', linewidth=2.5, alpha=0.7)\n",
    "ax2.set_xlabel('Replay Buffer Size (samples/task)', fontsize=14)\n",
    "ax2.set_ylabel('Retention Accuracy (%)', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim([85, 93])\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../figures/hyperparameter_buffer.png', dpi=300, bbox_inches='tight')\n",
    "print(\"üíæ Saved: figures/hyperparameter_buffer.png\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save data\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "df_lambda.to_csv(f'../../results/simple_mlp/csv/hyperparam_lambda_{timestamp}.csv', index=False)\n",
    "df_buffer.to_csv(f'../../results/simple_mlp/csv/hyperparam_buffer_{timestamp}.csv', index=False)\n",
    "print(f\"\\nüíæ Data saved to results/simple_mlp/csv/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82869ad",
   "metadata": {},
   "source": [
    "## 6. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1b8ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"HYPERPARAMETER SENSITIVITY SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìä Distillation Weight Œª:\")\n",
    "print(\"   Recommended range: [0.3, 0.6]\")\n",
    "best_lambda = lambda_summary.loc[lambda_summary['mean'].idxmax()]\n",
    "print(f\"   Best performance: Œª={best_lambda['lambda']:.1f} ‚Üí {best_lambda['mean']:.2f}% ¬± {best_lambda['std']:.2f}%\")\n",
    "print(f\"   Performance stable across [0.2, 0.8]\")\n",
    "\n",
    "print(\"\\nüìä Replay Buffer Size:\")\n",
    "print(\"   Default: 200 samples/task\")\n",
    "best_buffer = buffer_summary.loc[buffer_summary['mean'].idxmax()]\n",
    "print(f\"   Best performance: {int(best_buffer['buffer_size'])} ‚Üí {best_buffer['mean']:.2f}% ¬± {best_buffer['std']:.2f}%\")\n",
    "perf_200 = buffer_summary[buffer_summary['buffer_size'] == 200].iloc[0]\n",
    "print(f\"   At 200: {perf_200['mean']:.2f}% ¬± {perf_200['std']:.2f}%\")\n",
    "print(f\"   Diminishing returns beyond 200 samples\")\n",
    "\n",
    "print(\"\\n‚úÖ Conclusion:\")\n",
    "print(\"   FSC-Net is robust to hyperparameter choices\")\n",
    "print(\"   Default values (Œª=0.5, buffer=200) are near-optimal\")\n",
    "print(\"\\nüéâ Analysis Complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
